{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "671c63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# vis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "adff9c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed791c10",
   "metadata": {},
   "source": [
    "## Generate Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2c40478f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6334 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-01696c75d626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create path column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StudyInstanceUID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/*/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# train directory\n",
    "TRAIN_DIR = \"../input/train_study_samples/\"\n",
    "# paths\n",
    "paths = list()\n",
    "# create path column\n",
    "for _id in tqdm(train['StudyInstanceUID']):\n",
    "    paths.append(glob.glob(os.path.join(TRAIN_DIR, _id +\"/*/*\"))[0])   \n",
    "train['path'] = paths\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7a2f0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2a41bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    train_pcent = 0.8\n",
    "    TRAIN_BS = 32\n",
    "    VALID_BS = 16\n",
    "    NB_EPOCHS = 1\n",
    "    model_name = 'NN'\n",
    "    reshape_size = (400, 400)\n",
    "    num_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9e544",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "70a7d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIM(Dataset):\n",
    "    def __init__(self, df, is_train=True, augments=None, \n",
    "                 reshape_size=Config.reshape_size):\n",
    "        super().__init__()\n",
    "        # random sample data\n",
    "        self.df = df.sample(frac=1).reset_index(drop=True)\n",
    "        # training or validation\n",
    "        self.is_train = is_train\n",
    "        # augmentations\n",
    "        self.augments = augments\n",
    "        self.reshape_size = reshape_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def dicom2array(path, voi_lut=True, fix_monochrome=True):\n",
    "        dicom = pydicom.read_file(path)\n",
    "        # VOI LUT (if available by DICOM device) is used to\n",
    "        # transform raw DICOM data to \"human-friendly\" view\n",
    "        if voi_lut:\n",
    "            data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "        else:\n",
    "            data = dicom.pixel_array\n",
    "        # depending on this value, X-ray may look inverted - fix that:\n",
    "        if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "            data = np.amax(data) - data\n",
    "        data = data - np.min(data)\n",
    "        data = data / np.max(data)\n",
    "        data = (data * 255).astype(np.uint8)\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # retrieve image\n",
    "        image_id = self.df['StudyInstanceUID'].values[idx]\n",
    "        image_path = self.df['path'].values[idx]\n",
    "        # get image\n",
    "        image = self.dicom2array(image_path)\n",
    "        # recolour\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        # resize\n",
    "        image = cv2.resize(image, self.reshape_size)\n",
    "        # Augments\n",
    "        if self.augments:\n",
    "            image = self.augments(image=image)\n",
    "        else:\n",
    "            image = torch.tensor(image)   \n",
    "        # if train\n",
    "        if self.is_train:\n",
    "            label = self.df[self.df['StudyInstanceUID'] == image_id].values.tolist()[0][4:-1]\n",
    "            return image, torch.tensor(label)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c1f70",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Fake model function that should output a value of 1 for either:\n",
    "\n",
    "<code>'negative', 'typical', 'indeterminate', 'atypical'</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4bb55fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(NN, self).__init__()\n",
    "        # model spec\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.convolutions = nn.Sequential(nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
    "                                          *(list(resnet18().children())[1:-1]))\n",
    "        self.dense = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # batch size\n",
    "        batch_size = X.shape[0]\n",
    "        # create fake data\n",
    "        rand = torch.randn([batch_size, 4])\n",
    "        return Variable(self.out(rand), requires_grad=True)\n",
    "    \n",
    "    \n",
    "class EfficientNETB2(nn.Module):\n",
    "    def __init__(self, NUM_CLASSES=Config.num_classes):\n",
    "        super(EfficientNETB2, self).__init__()\n",
    "        # model\n",
    "        self.effnet = self.load_effnet(pretrained)\n",
    "        self.effnet._fc = nn.Linear(1408, self.NUM_CLASSES)\n",
    "        self.out = nn.Sigmoid()\n",
    "        \n",
    "    def load_effnet(self, pretrained):\n",
    "        if pretrained == True:\n",
    "            effnet = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "        else:\n",
    "            effnet = EfficientNet.from_name(\"efficientnet-b2\")\n",
    "        return effnet\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.effnet(X)\n",
    "        output = self.out(X)\n",
    "        return()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f038ff59",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "abf2b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataloader, valid_dataloader, model,\n",
    "                optimiser, loss_fn, val_loss_fn, device='cpu'):\n",
    "        \"\"\"\n",
    "        Constructor for Trainer class\n",
    "        \"\"\"\n",
    "        self.train = train_dataloader\n",
    "        self.valid = valid_dataloader\n",
    "        self.model = model\n",
    "        self.optim = optimiser\n",
    "        self.loss_fn = loss_fn\n",
    "        self.val_loss_fn = val_loss_fn\n",
    "        self.device = device\n",
    "        \n",
    "    def train_one_cycle(self):\n",
    "        \"\"\"\n",
    "        Run one epoch of training, backpropogation and optimisation.\n",
    "        \"\"\"\n",
    "            \n",
    "        # model train mode\n",
    "        model.train()\n",
    "        \n",
    "        # progress bar\n",
    "        train_prog_bar = tqdm(self.train, total=len(self.train))\n",
    "        \n",
    "        # stats\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "        running_loss = 0\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            for xtrain, ytrain in train_prog_bar:\n",
    "                # send to devices\n",
    "                xtrain = xtrain.to(self.device).float()\n",
    "                ytrain = ytrain.to(self.device).float()\n",
    "                \n",
    "                # get predictions\n",
    "                pred = model(xtrain)\n",
    "                # training\n",
    "                train_loss = self.loss_fn(pred, ytrain)\n",
    "\n",
    "                # Backpropogation\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                train_loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "                # For averaging and reporting later\n",
    "                running_loss += train_loss\n",
    "                \n",
    "                # convert predictions to numpy\n",
    "                train_predictions = torch.argmax(pred, 1).detach().cpu().numpy()\n",
    "                train_labels = ytrain.detach().cpu().numpy()\n",
    "                \n",
    "                # append to stats\n",
    "                all_train_labels += [train_predictions]\n",
    "                all_train_preds += [train_labels]\n",
    "\n",
    "                # show the current loss to the progress bar\n",
    "                train_pbar_desc = f'loss: {train_loss.item():.4f}'\n",
    "                train_prog_bar.set_description(desc=train_pbar_desc)\n",
    "                \n",
    "            # average the running loss over all batches and return\n",
    "            train_running_loss = running_loss / len(self.train)\n",
    "            print(f\"Final Training Loss: {train_running_loss:.4f}\")\n",
    "\n",
    "            # free memory\n",
    "            del all_train_labels, all_train_preds, train_predictions, train_labels, xtrain, ytrain, pred\n",
    "            # free up cache\n",
    "            torch.cuda.empty_cache()\n",
    "                \n",
    "            return(train_running_loss)\n",
    "        \n",
    "\n",
    "    def valid_one_cycle(self):\n",
    "        \"\"\"\n",
    "        Run one epoch of prediction.\n",
    "        \"\"\"\n",
    "            \n",
    "        # model eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        # progress bar\n",
    "        valid_prog_bar = tqdm(self.valid, total=len(self.train))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # stats\n",
    "            all_valid_labels = []\n",
    "            all_valid_preds = []\n",
    "            running_loss = 0\n",
    "            \n",
    "            for xval, yval in valid_prog_bar:\n",
    "                \n",
    "                # send to devices\n",
    "                xval = xval.to(self.device).float()\n",
    "                yval = yval.to(self.device).float()\n",
    "\n",
    "                # get predictions\n",
    "                pred = model(xval)\n",
    "\n",
    "                # training\n",
    "                val_loss = self.val_loss_fn(pred, yval)\n",
    "    \n",
    "                # For averaging and reporting later\n",
    "                running_loss += val_loss.item()\n",
    "                \n",
    "                # convert predictions to numpy\n",
    "                val_pred = torch.argmax(pred, 1).detach().cpu().numpy()\n",
    "                val_label = yval.detach().cpu().numpy()\n",
    "                \n",
    "                # append to stats\n",
    "                all_valid_labels += [val_label]\n",
    "                all_valid_preds += [val_pred]\n",
    "\n",
    "                # show the current loss to the progress bar\n",
    "                valid_pbar_desc = f'loss: {val_loss.item():.4f}'\n",
    "                valid_prog_bar.set_description(desc=valid_pbar_desc)\n",
    "                \n",
    "                # average the running loss over all batches and return\n",
    "                final_loss_val = running_loss / len(self.train)\n",
    "                \n",
    "                # Get Validation Accuracy\n",
    "                all_valid_labels = np.concatenate(all_valid_labels)\n",
    "                all_valid_preds = np.concatenate(all_valid_preds)\n",
    "                  \n",
    "                \n",
    "            print(f\"Final Validation Loss: {final_loss_val:.4f}\")                \n",
    "\n",
    "            # Free up memory\n",
    "            del all_valid_labels, all_valid_preds, val_label, val_pred, xval, yval, pred\n",
    "            # free cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return(final_loss_val, model)            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f9d19",
   "metadata": {},
   "source": [
    "# Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ac98bb7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training on 0 samples (80%) and validation on 1 (20%) samples\n",
      "\n",
      "[INFO] GPU not found. Using CPU: i386\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-7ef924e43329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m train_loader = DataLoader(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_BS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ml/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ml/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    104\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Training Cycle\n",
    "nb_training_samples = int(Config.train_pcent * train.id.shape[0])\n",
    "train_data = train[:nb_training_samples]\n",
    "valid_data = train[nb_training_samples:]\n",
    "\n",
    "print(f\"[INFO] Training on {train_data.shape[0]} samples ({int(Config.train_pcent*100)}%) and validation on {valid_data.shape[0]} ({ceil(abs(1 - Config.train_pcent) * 100)}%) samples\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "        DEVICE = torch.device('cuda:0')\n",
    "else:\n",
    "    print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n",
    "    DEVICE = torch.device('cpu')\n",
    "        \n",
    "\n",
    "# Make Training and Validation Datasets\n",
    "training_set = SIIM(\n",
    "    df=train_data\n",
    ")\n",
    "\n",
    "validation_set = SIIM(\n",
    "    df=valid_data\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=Config.TRAIN_BS,\n",
    "    shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=Config.VALID_BS,\n",
    "    shuffle=False)\n",
    "\n",
    "if \"EfficientNETB2\" in Config.model_name in Config.model_name:        \n",
    "    model = EfficientNETB2().to(DEVICE)\n",
    "    \n",
    "else:\n",
    "    raise RuntimeError(\"Must specify a valid model type to train.\")\n",
    "\n",
    "print(f\"Training Model: {Config.model_name}\")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "loss_fn_train = nn.BCELoss()\n",
    "loss_fn_val = nn.BCELoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=valid_loader,\n",
    "    model=model,\n",
    "    optimiser=optim,\n",
    "    loss_fn=loss_fn_train,\n",
    "    val_loss_fn=loss_fn_val,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "train_losses_eff = []\n",
    "valid_losses_eff = []\n",
    "\n",
    "for epoch in range(Config.NB_EPOCHS):\n",
    "    print(f\"{'-'*20} EPOCH: {epoch+1}/{Config.NB_EPOCHS} {'-'*20}\")\n",
    "\n",
    "    # Run one training epoch\n",
    "    current_train_loss = trainer.train_one_cycle()\n",
    "    train_losses_eff.append(current_train_loss)\n",
    "\n",
    "    # Run one validation epoch\n",
    "    current_val_loss, op_model = trainer.valid_one_cycle()\n",
    "    valid_losses_eff.append(current_val_loss)\n",
    "\n",
    "    # Empty CUDA cache\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
