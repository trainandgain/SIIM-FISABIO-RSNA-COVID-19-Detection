{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "peripheral-testing",
   "metadata": {},
   "source": [
    "# FasterRCNNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-jefferson",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection import  FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "# Albumenatations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2, ToTensor\n",
    "# numba\n",
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "# vis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "from math import ceil\n",
    "import cv2\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-winter",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "train = pd.read_csv('../input/train_exploded_filled.csv')\n",
    "id_s = pd.read_csv('../input/id_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    train_pcent = 0.8\n",
    "    TRAIN_BS = 2\n",
    "    VALID_BS = 2\n",
    "    NB_EPOCHS = 10\n",
    "    model_name = 'FasterRCNNDetector'\n",
    "    reshape_size = (800, 800)\n",
    "    num_classes = 4\n",
    "    seed = 69\n",
    "    iou_threshold = [0.5]\n",
    "    mean = 0.532\n",
    "    std = 0.208\n",
    "    max_pixel_value = 1.0\n",
    "    val_fold = 4\n",
    "    save_path = 'C:\\\\Users\\\\Admin\\\\Git\\\\SIIM\\\\models\\\\FasterRCNNDetector\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {attribute: value for attribute, value in Config.__dict__.items() if '__' not in attribute }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0: 'Negative for Pneumonia',\n",
    "1: 'Typical Appearance',\n",
    "2: 'Indeterminate Appearance',\n",
    "3: 'Atypical Appearance'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.val_fold = 4\n",
    "train_ids = id_s[id_s.fold!=4].id.values\n",
    "valid_ids = id_s[id_s.fold==4].id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-crack",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5), \n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4,\n",
    "                                  contrast_limit=0.4, p=0.3),\n",
    "        A.RandomGamma(gamma_limit=(50, 300), eps=None, always_apply=False, \n",
    "                      p=0.3),\n",
    "        A.Resize(height=Config.reshape_size[0], width=Config.reshape_size[1], \n",
    "                 p=1.0),\n",
    "        ToTensorV2(p=1.0)],\n",
    "        bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "valid_transform = A.Compose([A.Resize(height=Config.reshape_size[0], \n",
    "                                      width=Config.reshape_size[1], p=1.0),\n",
    "                             ToTensorV2(p=1.0)],\n",
    "        bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-hometown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIIM(Dataset):\n",
    "    def __init__(self, image_ids, df, transforms=None):\n",
    "        super().__init__()\n",
    "        # image_ids\n",
    "        self.image_ids = image_ids\n",
    "        # random sample data\n",
    "        self.df = df\n",
    "        # augmentations\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return(len(self.image_ids))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dicom2array(path: str, voi_lut=True, fix_monochrome=True):\n",
    "        dicom = pydicom.read_file(path)\n",
    "        # VOI LUT (if available by DICOM device) is used to\n",
    "        # transform raw DICOM data to \"human-friendly\" view\n",
    "        if voi_lut:\n",
    "            data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "        else:\n",
    "            data = dicom.pixel_array\n",
    "        # depending on this value, X-ray may look inverted - fix that:\n",
    "        if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "            data = np.amax(data) - data\n",
    "        data = data - np.min(data)\n",
    "        data = data / np.max(data)\n",
    "        return data.astype(np.float32)\n",
    "    \n",
    "    def load_bbox_labels(self, image_id, shape):\n",
    "        row, col = shape\n",
    "        records = self.df[self.df['id'] == image_id]\n",
    "        boxes = []\n",
    "        for bbox in records[['x', 'y', 'width', 'height']].values:\n",
    "            # get xmin, ymin, xmax, ymax\n",
    "            to_append = np.clip([bbox[0]/col, bbox[1]/row, (bbox[0]+bbox[2])/col, (bbox[1]+bbox[3])/row], 0, 1.0)\n",
    "            temp = A.convert_bbox_from_albumentations(to_append, 'pascal_voc', rows=row, cols=col) \n",
    "            boxes.append(temp)\n",
    "            #boxes.append(bbox)\n",
    "        labels = records['integer_label'].values\n",
    "        return(boxes, labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        # retrieve idx data\n",
    "        image_id = self.image_ids[idx]\n",
    "        # get path\n",
    "        image_path = self.df[self.df['id'] == image_id].file_path.values[0]\n",
    "        # get image\n",
    "        image = self.dicom2array(image_path)\n",
    "        # get boxes and labels\n",
    "        boxes, labels = self.load_bbox_labels(image_id, image.shape)\n",
    "        # target\n",
    "        target = {\n",
    "            'bboxes': boxes,\n",
    "            'labels': torch.tensor(labels)\n",
    "        }\n",
    "        # Augments\n",
    "        if self.transforms:\n",
    "            t = self.transforms(**{'image': image,\n",
    "                    'bboxes': target['bboxes'],\n",
    "                    'labels': target['labels']})\n",
    "            image = t['image']\n",
    "            t_bboxes = torch.stack(tuple(map(torch.tensor, \n",
    "                                             zip(*t['bboxes'])))).permute(1, 0)\n",
    "            target = {'boxes': t_bboxes,\n",
    "                      'labels': torch.tensor(labels)}\n",
    "\n",
    "        return image, target, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-klein",
   "metadata": {},
   "source": [
    "### Showing Output of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare database\n",
    "dataset = SIIM(image_ids=train_ids, df=train, transforms=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one(image, target):\n",
    "    print(target)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    boxes = target['boxes'].numpy().astype(np.int32)\n",
    "    image = image.squeeze(0).numpy()\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        startX, endX, startY, endY = int(xmin), int(xmax), int(ymin), int(ymax)\n",
    "        color = (0, 255, 0)\n",
    "        thickness = 1\n",
    "        image = cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.bone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-mouse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (image, target, idx) in enumerate(dataset):\n",
    "    if i % 2 ==0:\n",
    "        continue\n",
    "    if i > 20:\n",
    "        break\n",
    "    print(f'Showing image with id: {idx}')\n",
    "    show_one(image, target)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-police",
   "metadata": {},
   "source": [
    "### Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets, idx = tuple(zip(*batch))\n",
    "    return(tuple((torch.stack(images).float(), targets, idx)))\n",
    "    #return (tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-scale",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn, Tensor\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from torchvision.models.detection.roi_heads import paste_masks_in_image\n",
    "\n",
    "def resize_keypoints(keypoints, original_size, new_size):\n",
    "    # type: (Tensor, List[int], List[int]) -> Tensor\n",
    "    ratios = [\n",
    "        torch.tensor(s, dtype=torch.float32, device=keypoints.device) /\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=keypoints.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_h, ratio_w = ratios\n",
    "    resized_data = keypoints.clone()\n",
    "    if torch._C._get_tracing_state():\n",
    "        resized_data_0 = resized_data[:, :, 0] * ratio_w\n",
    "        resized_data_1 = resized_data[:, :, 1] * ratio_h\n",
    "        resized_data = torch.stack((resized_data_0, resized_data_1, resized_data[:, :, 2]), dim=2)\n",
    "    else:\n",
    "        resized_data[..., 0] *= ratio_w\n",
    "        resized_data[..., 1] *= ratio_h\n",
    "    return resized_data\n",
    "\n",
    "\n",
    "def resize_boxes(boxes, original_size, new_size):\n",
    "    # type: (Tensor, List[int], List[int]) -> Tensor\n",
    "    ratios = [\n",
    "        torch.tensor(s, dtype=torch.float32, device=boxes.device) /\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "class Identity2(nn.Module):\n",
    "    def __init__(self, size_divisible=32):\n",
    "        super(Identity2, self).__init__()\n",
    "        self.size_divisible = size_divisible\n",
    "        \n",
    "    def forward(self,\n",
    "                images,       # type: List[Tensor]\n",
    "                targets=None  # type: Optional[List[Dict[str, Tensor]]]\n",
    "                ):\n",
    "        # type: (...) -> Tuple[ImageList, Optional[List[Dict[str, Tensor]]]]\n",
    "        images = [img for img in images]\n",
    "        if targets is not None:\n",
    "            # make a copy of targets to avoid modifying it in-place\n",
    "            # once torchscript supports dict comprehension\n",
    "            # this can be simplified as follows\n",
    "            # targets = [{k: v for k,v in t.items()} for t in targets]\n",
    "            targets_copy: List[Dict[str, Tensor]] = []\n",
    "            for t in targets:\n",
    "                data: Dict[str, Tensor] = {}\n",
    "                for k, v in t.items():\n",
    "                    data[k] = v\n",
    "                targets_copy.append(data)\n",
    "            targets = targets_copy\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            target_index = targets[i] if targets is not None else None\n",
    "\n",
    "            if image.dim() != 3:\n",
    "                raise ValueError(\"images is expected to be a list of 3d tensors \"\n",
    "                                 \"of shape [C, H, W], got {}\".format(image.shape))\n",
    "            images[i] = image\n",
    "            if targets is not None and target_index is not None:\n",
    "                targets[i] = target_index\n",
    "\n",
    "        image_sizes = [img.shape[-2:] for img in images]\n",
    "        images = self.batch_images(images, size_divisible=self.size_divisible)\n",
    "        image_sizes_list: List[Tuple[int, int]] = []\n",
    "        for image_size in image_sizes:\n",
    "            assert len(image_size) == 2\n",
    "            image_sizes_list.append((image_size[0], image_size[1]))\n",
    "\n",
    "        image_list = ImageList(images, image_sizes_list)\n",
    "        return image_list, targets\n",
    "    \n",
    "    def batch_images(self, images, size_divisible=32):\n",
    "        # type: (List[Tensor], int) -> Tensor\n",
    "        if torchvision._is_tracing():\n",
    "            # batch_images() does not export well to ONNX\n",
    "            # call _onnx_batch_images() instead\n",
    "            return self._onnx_batch_images(images, size_divisible)\n",
    "\n",
    "        max_size = self.max_by_axis([list(img.shape) for img in images])\n",
    "        stride = float(size_divisible)\n",
    "        max_size = list(max_size)\n",
    "        max_size[1] = int(math.ceil(float(max_size[1]) / stride) * stride)\n",
    "        max_size[2] = int(math.ceil(float(max_size[2]) / stride) * stride)\n",
    "\n",
    "        batch_shape = [len(images)] + max_size\n",
    "        batched_imgs = images[0].new_full(batch_shape, 0)\n",
    "        for img, pad_img in zip(images, batched_imgs):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "\n",
    "        return batched_imgs\n",
    "    \n",
    "    def max_by_axis(self, the_list):\n",
    "        # type: (List[List[int]]) -> List[int]\n",
    "        maxes = the_list[0]\n",
    "        for sublist in the_list[1:]:\n",
    "            for index, item in enumerate(sublist):\n",
    "                maxes[index] = max(maxes[index], item)\n",
    "        return maxes\n",
    "    \n",
    "    def postprocess(self,\n",
    "                    result,               # type: List[Dict[str, Tensor]]\n",
    "                    image_shapes,         # type: List[Tuple[int, int]]\n",
    "                    original_image_sizes  # type: List[Tuple[int, int]]\n",
    "                    ):\n",
    "        # type: (...) -> List[Dict[str, Tensor]]\n",
    "        if self.training:\n",
    "            return result\n",
    "        for i, (pred, im_s, o_im_s) in enumerate(zip(result, image_shapes, original_image_sizes)):\n",
    "            boxes = pred[\"boxes\"]\n",
    "            boxes = resize_boxes(boxes, im_s, o_im_s)\n",
    "            result[i][\"boxes\"] = boxes\n",
    "            if \"masks\" in pred:\n",
    "                masks = pred[\"masks\"]\n",
    "                masks = paste_masks_in_image(masks, boxes, o_im_s)\n",
    "                result[i][\"masks\"] = masks\n",
    "            if \"keypoints\" in pred:\n",
    "                keypoints = pred[\"keypoints\"]\n",
    "                keypoints = resize_keypoints(keypoints, im_s, o_im_s)\n",
    "                result[i][\"keypoints\"] = keypoints\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        _indent = '\\n    '\n",
    "        format_string += \"Custom version of GeneralizedRCNNTransform without resizing or norms...\"\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNDetector(torch.nn.Module):\n",
    "    def __init__(self, pretrained=False, **kwargs):\n",
    "        super(FasterRCNNDetector, self).__init__()\n",
    "        # load pre-trained model incl. head\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained, \n",
    "                                                                          pretrained_backbone=pretrained)\n",
    "        # change to 1 channel input\n",
    "        self.model.backbone.body.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), \n",
    "                                                   stride=(2, 2), padding=(3, 3), \n",
    "                                                   bias=False)\n",
    "        # self.model.backbone\n",
    "        self.model.transform = Identity2()\n",
    "        # get number of input features for the classifier custom head\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, Config.num_classes)\n",
    "        \n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-thumb",
   "metadata": {},
   "source": [
    "## Image Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.kaggle.com/pestipeti/competition-metric-details-script\n",
    "'''\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_iou(gt, pr, form='pascal_voc') -> float:\n",
    "    \"\"\"Calculates the Intersection over Union.\n",
    "\n",
    "    Args:\n",
    "        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n",
    "        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n",
    "        form: (str) gt/pred coordinates format\n",
    "            - pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "            - coco: [xmin, ymin, w, h]\n",
    "    Returns:\n",
    "        (float) Intersection over union (0.0 <= iou <= 1.0)\n",
    "    \"\"\"\n",
    "    if form == 'coco':\n",
    "        gt = gt.copy()\n",
    "        pr = pr.copy()\n",
    "\n",
    "        gt[2] = gt[0] + gt[2]\n",
    "        gt[3] = gt[1] + gt[3]\n",
    "        pr[2] = pr[0] + pr[2]\n",
    "        pr[3] = pr[1] + pr[3]\n",
    "\n",
    "    # Calculate overlap area\n",
    "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
    "    \n",
    "    if dx < 0:\n",
    "        return 0.0\n",
    "    \n",
    "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
    "\n",
    "    if dy < 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_area = dx * dy\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = (\n",
    "            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n",
    "            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n",
    "            overlap_area\n",
    "    )\n",
    "\n",
    "    return overlap_area / union_area\n",
    "\n",
    "@jit(nopython=True)\n",
    "def find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n",
    "    \"\"\"Returns the index of the 'best match' between the\n",
    "    ground-truth boxes and the prediction. The 'best match'\n",
    "    is the highest IoU. (0.0 IoUs are ignored).\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        pred: (List[Union[int, float]]) Coordinates of the predicted box\n",
    "        pred_idx: (int) Index of the current predicted box\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (int) Index of the best match GT box (-1 if no match above threshold)\n",
    "    \"\"\"\n",
    "    best_match_iou = -np.inf\n",
    "    best_match_idx = -1\n",
    "\n",
    "    for gt_idx in range(len(gts)):\n",
    "        \n",
    "        if gts[gt_idx][0] < 0:\n",
    "            # Already matched GT-box\n",
    "            continue\n",
    "        \n",
    "        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n",
    "\n",
    "        if iou < 0:\n",
    "            iou = calculate_iou(gts[gt_idx], pred, form=form)\n",
    "            \n",
    "            if ious is not None:\n",
    "                ious[gt_idx][pred_idx] = iou\n",
    "\n",
    "        if iou < threshold:\n",
    "            continue\n",
    "\n",
    "        if iou > best_match_iou:\n",
    "            best_match_iou = iou\n",
    "            best_match_idx = gt_idx\n",
    "\n",
    "    return best_match_idx\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n",
    "    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n = len(preds)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # for pred_idx, pred in enumerate(preds_sorted):\n",
    "    for pred_idx in range(n):\n",
    "\n",
    "        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n",
    "                                            threshold=threshold, form=form, ious=ious)\n",
    "\n",
    "        if best_match_gt_idx >= 0:\n",
    "            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n",
    "            tp += 1\n",
    "            # Remove the matched GT box\n",
    "            gts[best_match_gt_idx] = -1\n",
    "\n",
    "        else:\n",
    "            # No match\n",
    "            # False positive: indicates a predicted box had no associated gt box.\n",
    "            fp += 1\n",
    "\n",
    "    # False negative: indicates a gt box had no associated predicted box.\n",
    "    fn = (gts.sum(axis=1) > 0).sum()\n",
    "\n",
    "    return tp / (tp + fp + fn)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n",
    "    \"\"\"Calculates image precision.\n",
    "       The mean average precision at different intersection over union (IoU) thresholds.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        thresholds: (float) Different thresholds\n",
    "        form: (str) Format of the coordinates\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n_threshold = len(thresholds)\n",
    "    image_precision = 0.0\n",
    "    \n",
    "    ious = np.ones((len(gts), len(preds))) * -1\n",
    "    # ious = None\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n",
    "                                                     form=form, ious=ious)\n",
    "        image_precision += precision_at_threshold / n_threshold\n",
    "\n",
    "    return image_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-hierarchy",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logman(object):\n",
    "    \"\"\"\n",
    "    Json data should save as follows:\n",
    "    \n",
    "    {'type': hyper, 'model':?, 'optim': ?, 'model': ?, 'train_len': ?, 'val_len': ?, 'device': ? ...etc...}\n",
    "\n",
    "    {'type': train, 'epoch': num, 'batch': num, 'loss': num, ...etc...}\n",
    "\n",
    "    {'type': val, 'epoch': num, 'batch': num, 'loss': num, 'metric': num ...etc...}\n",
    "\n",
    "    {'type': final, 'epochs': num, 'batches': num, 'final_loss': num, 'final_metric': num ...etc...}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hyper, save_path='./', save_name='logs'):\n",
    "        #load\n",
    "        self.hyper = hyper\n",
    "        self.save_path = save_path\n",
    "        self.save_name = save_name\n",
    "        # assertions\n",
    "        assert self.hyper['type'] == 'hyper'\n",
    "        assert self.hyper['model']\n",
    "        # init store\n",
    "        self.store = {'model': self.hyper['model']}\n",
    "        self.store.update({k: v for k, v in self.hyper.items() if (k != 'type' and k != 'model')})\n",
    "        self.store['data'] = []\n",
    "        # save empty logs\n",
    "        self.save_logs()\n",
    "        \n",
    "    def save_logs(self):\n",
    "        \"\"\"\n",
    "        The current implementation does not append or concatenate current file\n",
    "        but instead save on top of current file with large dictionary.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.save_path, self.save_name)+'.json','w') as file:\n",
    "            json.dump(self.store, file, indent = 4)\n",
    "            file.close()\n",
    "            \n",
    "    def log(self, data):\n",
    "        \"\"\"\n",
    "        Takes in any input data of form dict.\n",
    "        Handles data by key 'type'.\n",
    "        Sends to relevant method.\n",
    "        \"\"\"\n",
    "        if data:\n",
    "            # first log\n",
    "            if not self.store['data']:\n",
    "                self.store['data'] = [data]\n",
    "            else:\n",
    "                # check for final\n",
    "                if data['type'] == 'final':\n",
    "                    self.finalise(data)\n",
    "                else:\n",
    "                    # append data to data key in store\n",
    "                    self.store['data'].append(data)\n",
    "        else:\n",
    "            # error\n",
    "            self.store['data'].append({'type': 'error', 'reason': 'No data present'})\n",
    "    \n",
    "    def finalise(self, data):\n",
    "        \"\"\"\n",
    "        Finalise logs, send final parameters to dict.\n",
    "        Save logs\n",
    "        \"\"\"\n",
    "        self.store.update({k: v for k, v in data.items() if (k != 'type' and k != 'model')})\n",
    "        self.save_logs()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-strain",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataloader, valid_dataloader, model,\n",
    "                optimiser, logger=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        Constructor for Trainer class\n",
    "        \"\"\"\n",
    "        self.train = train_dataloader\n",
    "        self.valid = valid_dataloader\n",
    "        self.model = model\n",
    "        self.optim = optimiser\n",
    "        self.logger = logger\n",
    "        self.device = device\n",
    "        \n",
    "    def train_one_cycle(self, epoch):\n",
    "        \"\"\"\n",
    "        Run one epoch of training, backpropogation and optimisation.\n",
    "        \"\"\"        \n",
    "        # model train mode\n",
    "        model.train()\n",
    "        \n",
    "        # progress bar\n",
    "        train_prog_bar = tqdm(self.train, total=len(self.train))\n",
    "        \n",
    "        # stats\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "        running_loss = 0\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            for batch_num, (images, targets, idx) in enumerate(train_prog_bar):\n",
    "                # zero gradient optim\n",
    "                self.optim.zero_grad()\n",
    "                \n",
    "                # send to devices\n",
    "                images = images.to(self.device)\n",
    "                tg = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                # get outputs\n",
    "                losses = model(images, tg)\n",
    "                # training\n",
    "                train_loss = sum(loss for loss in losses.values())\n",
    "\n",
    "                # Backpropogation\n",
    "                train_loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "                # For averaging and reporting later\n",
    "                running_loss += train_loss.item()\n",
    "                \n",
    "                # logging\n",
    "                if self.logger:\n",
    "                    self.logger.log({'type': 'train', \n",
    "                                     'epoch': epoch, \n",
    "                                     'batch': batch_num, \n",
    "                                     'loss': train_loss.item()\n",
    "                                    })\n",
    "                    \n",
    "                # show the current loss to the progress bar\n",
    "                train_pbar_desc = f'loss: {train_loss.item():.4f}'\n",
    "                train_prog_bar.set_description(desc=train_pbar_desc)\n",
    "                \n",
    "            # average the running loss over all batches and return\n",
    "            train_running_loss = running_loss / len(self.train)\n",
    "            print(f\"Final Training Loss: {train_running_loss:.4f}\")\n",
    "\n",
    "            # free memory\n",
    "            del images, tg, losses, train_loss\n",
    "            # free up cache\n",
    "            torch.cuda.empty_cache()\n",
    "            return(train_running_loss)\n",
    "        \n",
    "\n",
    "    def valid_one_cycle(self, epoch):\n",
    "        \"\"\"\n",
    "        Runs one epoch of prediction.\n",
    "        In model.train() mode, model(images)  is returning losses.\n",
    "        We are using model.eval() mode --> it will return boxes and scores. \n",
    "        \"\"\"        \n",
    "        model.eval()\n",
    "        \n",
    "        valid_prog_bar = tqdm(self.valid, total=len(self.valid))\n",
    "        count = 0\n",
    "        i_sum = 0\n",
    "        avg = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            metric = 0\n",
    "\n",
    "            for batch_num, (images, targets, idx) in enumerate(valid_prog_bar):\n",
    "                # send to devices\n",
    "                images = images.to(self.device)\n",
    "                \n",
    "                # get predictions\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # get metric\n",
    "                for i, image in enumerate(images):               \n",
    "                    gt_boxes = targets[i]['boxes'].data.cpu().numpy()\n",
    "                    boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "                    scores = outputs[i]['scores'].detach().cpu().numpy()\n",
    "                    \n",
    "                    preds_sorted_idx = np.argsort(scores)[::-1]\n",
    "                    preds_sorted_boxes = boxes[preds_sorted_idx]\n",
    "                    precision = calculate_image_precision(preds_sorted_boxes,\n",
    "                                                          gt_boxes,\n",
    "                                                          thresholds=Config.iou_threshold,\n",
    "                                                          form='pascal_voc')\n",
    "                    count += 1\n",
    "                    i_sum += precision * 1 # should be n but updating one at a time?\n",
    "                    avg = i_sum / count\n",
    "                    \n",
    "                    # logging\n",
    "                    if self.logger:\n",
    "                        self.logger.log({'type': 'val', \n",
    "                                         'epoch': epoch, \n",
    "                                         'batch': batch_num, \n",
    "                                         'image_precision': precision,\n",
    "                                         'average_precision': avg\n",
    "                                        })\n",
    "                        \n",
    "                    # Show the current metric\n",
    "                    valid_pbar_desc = f\"Avg Precision: {avg:.4f}\"\n",
    "                    valid_prog_bar.set_description(desc=valid_pbar_desc)\n",
    "            \n",
    "            print(f\"Validation metric: {avg:.4f}\")\n",
    "\n",
    "            # Free up memory\n",
    "            del images, outputs, gt_boxes, boxes, scores, preds_sorted_idx, preds_sorted_boxes, precision\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return (avg, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-bahrain",
   "metadata": {},
   "source": [
    "## Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-sucking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "nb_training_samples = len(train_ids)\n",
    "nb_valid_samples = len(valid_ids)\n",
    "print(f\"[INFO] Training on {len(train_ids)} samples ({int(Config.train_pcent*100)}%) and validation on {nb_valid_samples} ({ceil(abs(1 - Config.train_pcent) * 100)}%) samples\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "        DEVICE = torch.device('cuda:0')\n",
    "else:\n",
    "    print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n",
    "    DEVICE = torch.device('cpu')\n",
    "################################################################################################################################        \n",
    "\n",
    "# Make Training and Validation Datasets\n",
    "training_set = SIIM(image_ids=train_ids, \n",
    "               df=train, transforms=train_transform)\n",
    "\n",
    "validation_set = SIIM(image_ids=valid_ids, \n",
    "               df=train, transforms=valid_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=Config.TRAIN_BS,\n",
    "    shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=Config.VALID_BS,\n",
    "    shuffle=False, collate_fn=collate_fn)\n",
    "################################################################################################################################        \n",
    "if \"FasterRCNNDetector\" in Config.model_name in Config.model_name:        \n",
    "    model = FasterRCNNDetector(pretrained=True).to(DEVICE)\n",
    "    \n",
    "else:\n",
    "    raise RuntimeError(\"Must specify a valid model type to train.\")\n",
    "\n",
    "print(f\"Training Model: {Config.model_name}\")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "optim_name = 'Adam'\n",
    "################################################################################################################################        \n",
    "params = {'type': 'hyper',\n",
    "          'model': Config.model_name,\n",
    "          'optim': optim_name,\n",
    "          'train_samples': nb_training_samples, \n",
    "          'val_samples': nb_valid_samples, \n",
    "          'device': torch.cuda.get_device_name()}\n",
    "params.update(config_dict)\n",
    "hyper_params = params\n",
    "logman = logman(hyper_params, Config.save_path, Config.model_name)\n",
    "################################################################################################################################        \n",
    "\n",
    "trainer = Trainer(\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=valid_loader,\n",
    "    model=model,\n",
    "    optimiser=optim,\n",
    "    logger=logman,\n",
    "    device=DEVICE\n",
    ")\n",
    "################################################################################################################################        \n",
    "\n",
    "train_losses_eff = []\n",
    "valid_prec = []\n",
    "\n",
    "for epoch in range(Config.NB_EPOCHS):\n",
    "    # Empty CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"{'-'*20} EPOCH: {epoch+1}/{Config.NB_EPOCHS} {'-'*20}\")\n",
    "\n",
    "    # Run one training epoch\n",
    "    current_train_loss = trainer.train_one_cycle(epoch)\n",
    "    train_losses_eff.append(current_train_loss)\n",
    "\n",
    "    # Run one validation epoch\n",
    "    current_precision, op_model = trainer.valid_one_cycle(epoch)\n",
    "    valid_prec.append(current_precision)\n",
    "    op_model.eval()\n",
    "    torch.save({\n",
    "        'model_state_dict': op_model.state_dict(), #'model_state_dict': self.model.model.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'loss': current_train_loss,\n",
    "        'image_precision': current_precision,\n",
    "        'epoch': epoch,\n",
    "        \n",
    "    }, Config.save_path+'epoch_{epoch}.pth'.format(epoch=epoch))\n",
    "\n",
    "logman.log({'type': 'final', \n",
    "            'final_loss': current_train_loss, \n",
    "            'final_metric': current_precision})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharex = True, figsize=(15, 10))\n",
    "ax.plot(np.arange(0, len(train_losses_eff), 1), train_losses_eff, label='Train')\n",
    "ax.plot(np.arange(0, len(valid_prec), 1), valid_prec, label='Valid')\n",
    "ax.set_xlabel('# of epochs')\n",
    "ax.legend()\n",
    "ax.set_title('Loss and Precision over epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cell]",
   "language": "python",
   "name": "conda-env-cell-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
